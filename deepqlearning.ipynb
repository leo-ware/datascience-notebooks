{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning for FrozenLake-v0\n",
    "\n",
    "This is a very similar project to what I did in qlearning.ipynb, but I use deep instead of tabular Q-Learning. To do this, I use the keras-rl package. To do this, I first design a vanilla, supervised network in Keras, and then I train it using the same updates as in tabular Q-Learning. This way, the neural net (hopefully) memorizes the data that would have been contained in the Q-Table.\n",
    "\n",
    "Important: keras-rl only works with tensorflow v <= 1.14. You will probably have to downgrade or use a virtual environment to use the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(16, input_shape=[16], activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(model=model, nb_actions=env.action_space.n, memory=SequentialMemory(10000, window_length=16))\n",
    "agent.compile(Adam(lr=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "2500/2500 [==============================] - 12s 5ms/step - reward: 4.0000e-04\n",
      "187 episodes - episode_reward: 0.005 [0.000, 1.000] - loss: 0.050 - mae: 0.146 - mean_q: 0.670 - prob: 0.333\n",
      "\n",
      "Interval 2 (2500 steps performed)\n",
      "2500/2500 [==============================] - 18s 7ms/step - reward: 4.0000e-04\n",
      "199 episodes - episode_reward: 0.005 [0.000, 1.000] - loss: 0.047 - mae: 0.162 - mean_q: 0.623 - prob: 0.333\n",
      "\n",
      "Interval 3 (5000 steps performed)\n",
      "2500/2500 [==============================] - 17s 7ms/step - reward: 0.0000e+00\n",
      "212 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.044 - mae: 0.191 - mean_q: 0.532 - prob: 0.333\n",
      "\n",
      "Interval 4 (7500 steps performed)\n",
      "2500/2500 [==============================] - 17s 7ms/step - reward: 4.0000e-04\n",
      "done, took 64.106 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13e5df6d8>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.fit(env, nb_steps=10000, verbose=1, log_interval=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 0.048\n"
     ]
    }
   ],
   "source": [
    "# see how well it plays\n",
    "\n",
    "rewards = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    env.reset()\n",
    "    done = False\n",
    "    observation, reward, done, info = env.step(0)\n",
    "\n",
    "    for _ in range(500):\n",
    "        action = agent.forward(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            rewards.append(reward)\n",
    "            break\n",
    "\n",
    "print(\"Average Score:\", sum(rewards)/len(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
